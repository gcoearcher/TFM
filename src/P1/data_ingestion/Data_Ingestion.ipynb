{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e16a4a9",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3426ea",
   "metadata": {},
   "source": [
    "## AWS Open Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8462f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"abeja-cc-ja\"\n",
    "output_folder = '../../../datasets/AWS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54416ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_aws_public_csv(bucket_name, object_key):\n",
    "    s3 = boto3.client('s3')\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    df = pd.read_csv(BytesIO(obj['Body'].read()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00def43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_aws_public_csv_in_chunks(bucket_name, object_key, chunksize=10**6):\n",
    "    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    stream = obj['Body']  # StreamingBody object\n",
    "    \n",
    "    # Return iterator of dataframes (chunks)\n",
    "    return pd.read_csv(stream, chunksize=1000, delimiter=';', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_objects(bucket_name):\n",
    "    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name)\n",
    "\n",
    "    keys = []\n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                keys.append(obj['Key'])\n",
    "    return keys\n",
    "\n",
    "\n",
    "all_keys = list_all_objects(bucket_name)\n",
    "\n",
    "print(f\"Found {len(all_keys)} files in the bucket.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: STOP HERE IF YOU DON'T WANT TO DOWNLOAD ALL FILES\n",
    "# Limited to 1 first keys \n",
    "for key in all_keys[:1]:\n",
    "    print(f\"Reading {key} in chunks ...\")\n",
    "    chunk_iter = read_aws_public_csv_in_chunks(bucket_name, key)\n",
    "    first_chunk = next(chunk_iter)  # get first chunk (a DataFrame)\n",
    "    print(first_chunk.head())       # now you can call head()   \n",
    "    base_filename = os.path.basename(key).replace('/', '_').replace('.gz', '')  # safe filename\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "    # Limited to 5 first chunks \n",
    "        if i >= 3:\n",
    "            break  # stop after 3 chunks\n",
    "\n",
    "        chunk_file = os.path.join(output_folder, f\"{base_filename}_chunk{i}.csv\")\n",
    "        chunk.to_csv(chunk_file, index=False)\n",
    "        print(f\"Saved chunk {i} of {key} to {chunk_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62c372",
   "metadata": {},
   "source": [
    "## Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20218228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not working first time, it works at second\n",
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f41aea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_links = [\n",
    "    'sahideseker/tweet-sentiment-classification-dataset',\n",
    "    'abdallahwagih/emotion-dataset',\n",
    "    'simaanjali/emotion-analysis-based-on-text',\n",
    "    'durgeshrao9993/twitter-analysis-dataset-2022',\n",
    "    'aadyasingh55/twitter-emotion-classification-dataset',\n",
    "    'mgmitesh/sentiment-analysis-dataset'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e289fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['KAGGLE_CONFIG_DIR'] = '/root/.config/kaggle/'\n",
    "import json\n",
    "\n",
    "#import json in kaggle/kaggle.json\n",
    "with open(\"../../../kaggle/kaggle.json\", \"r\") as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = creds['username']\n",
    "os.environ['KAGGLE_KEY'] = creds['key']\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc6fffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/sahideseker/tweet-sentiment-classification-dataset\n",
      "Dataset URL: https://www.kaggle.com/datasets/abdallahwagih/emotion-dataset\n",
      "Dataset URL: https://www.kaggle.com/datasets/simaanjali/emotion-analysis-based-on-text\n",
      "Dataset URL: https://www.kaggle.com/datasets/durgeshrao9993/twitter-analysis-dataset-2022\n",
      "Dataset URL: https://www.kaggle.com/datasets/aadyasingh55/twitter-emotion-classification-dataset\n",
      "Dataset URL: https://www.kaggle.com/datasets/mgmitesh/sentiment-analysis-dataset\n"
     ]
    }
   ],
   "source": [
    "for link in data_links:\n",
    "    files = api.dataset_list_files(link).files\n",
    "    extensions = {os.path.splitext(f.name)[1].lstrip('.') for f in files}\n",
    "    folder = next(iter(extensions), 'unknown')  \n",
    "    save_path = f'../../../datasets/Kaggle/{folder}'\n",
    "    api.dataset_download_files(link, path=save_path, unzip=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195e668",
   "metadata": {},
   "source": [
    "## UCI ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd56f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba494ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\"\n",
    "dir = \"../../../datasets/uci\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f27adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(local_filename, save_dir):\n",
    "    with zipfile.ZipFile(local_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d6228ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: ../../../datasets/uci/sentiment%20labelled%20sentences.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../../../datasets/uci/sentiment%20labelled%20sentences.zip'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def download_uci_dataset(url, save_dir=dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    local_filename = os.path.join(save_dir, url.split(\"/\")[-1])\n",
    "    response = requests.get(url)\n",
    "\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded: {local_filename}\")\n",
    "\n",
    "    unzip(local_filename, save_dir)\n",
    "    \n",
    "    return local_filename\n",
    "\n",
    "download_uci_dataset(url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_tfm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
