{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "143b6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql.functions import col, trim, lower, regexp_replace\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79bd2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exemple of path_data\n",
    "path_landing = \"../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_twitter_csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c19df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_connector_jar = \"/home/provira/Documents/TFM/TFM/src/P2/trusted_zone/jars/mongo-spark-connector_2.12-3.0.1.jar\"\n",
    "mongo_driver_jar = \"/home/provira/Documents/TFM/TFM/src/P2/trusted_zone/jars/mongo-java-driver-3.12.10.jar\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42649ad0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f5325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/27 21:25:16 WARN Utils: Your hostname, provira-ERAZER-P6705-MD61203 resolves to a loopback address: 127.0.1.1; using 192.168.1.55 instead (on interface wlo1)\n",
      "25/06/27 21:25:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/provira/anaconda3/envs/spark_py3.9/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/provira/anaconda3/envs/spark_py3.9/lib/python3.9/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/provira/.ivy2/cache\n",
      "The jars for the packages stored in: /home/provira/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5b3af7b3-49b1-4eb8-a9e5-632495fcd4ff;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      ":: resolution report :: resolve 428ms :: artifacts dl 39ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5b3af7b3-49b1-4eb8-a9e5-632495fcd4ff\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/11ms)\n",
      "25/06/27 21:25:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Trusted_Zone\") \\\n",
    "    .config(\"spark.jars\", f\"{mongo_connector_jar},{mongo_driver_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://localhost:27017\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://localhost:27017\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b7377f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Comment: string, Emotion: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._jsc.sc().listJars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e44e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFromDeltaLake(path_landing):\n",
    "    print(path_landing)\n",
    "    return spark.read.format(\"delta\").load(path_landing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89ea2c",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77053ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCSV(df_csv):\n",
    "    print(df_csv.columns)\n",
    "    df_csv_clean = df_csv.dropna()\n",
    "\n",
    "    rename_map = {\n",
    "        \"_c0\": \"id\",\n",
    "        \"Emotion\": \"emotion\",\n",
    "        \"sentiment\": \"emotion\",\n",
    "        \"tweet\": \"text\",\n",
    "        \"label\": \"emotion\",\n",
    "    }\n",
    "\n",
    "    for old, new in rename_map.items():\n",
    "        if old in df_csv_clean.columns:\n",
    "            df_csv_clean = df_csv_clean.withColumnRenamed(old, new)\n",
    "\n",
    "    # Only apply text processing if 'text' exists\n",
    "    if \"text\" in df_csv_clean.columns:\n",
    "        df_csv_clean = df_csv_clean \\\n",
    "            .withColumn(\"text\", trim(col(\"text\"))) \\\n",
    "            .withColumn(\"text\", lower(col(\"text\"))) \\\n",
    "            .withColumn(\"text\", regexp_replace(col(\"text\"), r\"\\bi m\\b\", \"i'm\")) \\\n",
    "            .withColumn(\"text\", regexp_replace(col(\"text\"), r\"[^a-zA-Z0-9\\s']\", \"\"))  # keep letters, digits, spaces, apostrophes\n",
    "    return df_csv_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b117aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "def tokenizer(df_csv_clean):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Tokenizar\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    df_words = tokenizer.transform(df_csv_clean) #.limit(1000) limited to 1000 rows for performance\n",
    "\n",
    "    # Eliminar stopwords (solo en ingl√©s por defecto, pero puedes pasar las tuyas)\n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "    remover.setStopWords(list(stop_words))\n",
    "    df_filtered = remover.transform(df_words)\n",
    "\n",
    "    #Stemming\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "\n",
    "    def stem_tokens(tokens):\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    stem_udf = udf(stem_tokens, ArrayType(StringType()))\n",
    "\n",
    "    df_stemmed = df_filtered.withColumn(\"stemmed_words\", stem_udf(\"filtered_words\"))\n",
    "\n",
    "    #df_stemmed.select(\"text\", \"filtered_words\", \"stemmed_words\").show(truncate=False)\n",
    "\n",
    "    return df_stemmed\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e21e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "\n",
    "def tf_idf(df_stemmed):\n",
    "\n",
    "    # Paso 1: Crear el CountVectorizer para extraer el vocabulario y conteo de tokens\n",
    "    cv = CountVectorizer(inputCol=\"stemmed_words\", outputCol=\"raw_features\")\n",
    "    cv_model = cv.fit(df_stemmed)             # Entrenas el modelo con el vocabulario\n",
    "    df_featurized = cv_model.transform(df_stemmed)  # Transformas el DataFrame\n",
    "\n",
    "    # Paso 2: Calcular TF-IDF a partir del conteo\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
    "    idf_model = idf.fit(df_featurized)          # Ajustar IDF sobre los datos\n",
    "    df_tfidf = idf_model.transform(df_featurized) # Transformar con TF-IDF\n",
    "\n",
    "    # Mostrar resultados\n",
    "    #df_tfidf.select(\"stemmed_words\", \"raw_features\", \"tfidf_features\").show(truncate=False)\n",
    "    df_tfidf.printSchema()\n",
    "\n",
    "    return df_tfidf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc0352",
   "metadata": {},
   "source": [
    "# Store in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56badaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "def df_clean(df_tfidf):\n",
    "    def vector_to_array(v):\n",
    "        return v.toArray().tolist() if v else None\n",
    "\n",
    "    vector_to_array_udf = udf(vector_to_array, ArrayType(FloatType()))\n",
    "\n",
    "    df_tfidf_safe = df_tfidf \\\n",
    "        .withColumn(\"raw_features_array\", vector_to_array_udf(\"raw_features\")) \\\n",
    "        .withColumn(\"tfidf_features_array\", vector_to_array_udf(\"tfidf_features\"))\n",
    "    return df_tfidf_safe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f9c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeInMongoDB(df_tfidf_safe):\n",
    "    df_tfidf_safe.select(\n",
    "        \"text\", \"Emotion\", \"words\", \"filtered_words\", \"stemmed_words\",\n",
    "        \"raw_features_array\", \"tfidf_features_array\"\n",
    "    ).write \\\n",
    "        .format(\"mongo\") \\\n",
    "        .option(\"uri\", \"mongodb://localhost:27017\") \\\n",
    "        .option(\"database\", \"tfm-trusted-zone\") \\\n",
    "        .option(\"collection\", \"tf-idf\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff479dcb",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9e72806",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanCSV_udf = udf(cleanCSV, ArrayType(StringType()))\n",
    "tokenizer_udf = udf(tokenizer, ArrayType(StringType()))\n",
    "df_clean_udf = udf(df_clean, ArrayType(StringType()))  # Update return type if it's different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b331f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['csv', 'parquet', 'json', 'txt']\n",
    "sources = ['Kaggle', 'uci', 'AWS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943c2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_GoEmotions_goemotions_3_csv/\n",
      "./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_GoEmotions_goemotions_3_csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id', 'created_utc', 'rater_id', 'example_very_unclear', 'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/27 21:25:40 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- rater_id: string (nullable = true)\n",
      " |-- example_very_unclear: string (nullable = true)\n",
      " |-- admiration: string (nullable = true)\n",
      " |-- amusement: string (nullable = true)\n",
      " |-- anger: string (nullable = true)\n",
      " |-- annoyance: string (nullable = true)\n",
      " |-- approval: string (nullable = true)\n",
      " |-- caring: string (nullable = true)\n",
      " |-- confusion: string (nullable = true)\n",
      " |-- curiosity: string (nullable = true)\n",
      " |-- desire: string (nullable = true)\n",
      " |-- disappointment: string (nullable = true)\n",
      " |-- disapproval: string (nullable = true)\n",
      " |-- disgust: string (nullable = true)\n",
      " |-- embarrassment: string (nullable = true)\n",
      " |-- excitement: string (nullable = true)\n",
      " |-- fear: string (nullable = true)\n",
      " |-- gratitude: string (nullable = true)\n",
      " |-- grief: string (nullable = true)\n",
      " |-- joy: string (nullable = true)\n",
      " |-- love: string (nullable = true)\n",
      " |-- nervousness: string (nullable = true)\n",
      " |-- optimism: string (nullable = true)\n",
      " |-- pride: string (nullable = true)\n",
      " |-- realization: string (nullable = true)\n",
      " |-- relief: string (nullable = true)\n",
      " |-- remorse: string (nullable = true)\n",
      " |-- sadness: string (nullable = true)\n",
      " |-- surprise: string (nullable = true)\n",
      " |-- neutral: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- stemmed_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- tfidf_features: vector (nullable = true)\n",
      "\n",
      "Error processing folder ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_GoEmotions_goemotions_3_csv: cannot resolve '`Emotion`' given input columns: [admiration, amusement, anger, annoyance, approval, author, caring, confusion, created_utc, curiosity, desire, disappointment, disapproval, disgust, embarrassment, example_very_unclear, excitement, fear, filtered_words, gratitude, grief, id, joy, link_id, love, nervousness, neutral, optimism, parent_id, pride, rater_id, raw_features, raw_features_array, realization, relief, remorse, sadness, stemmed_words, subreddit, surprise, text, tfidf_features, tfidf_features_array, words];\n",
      "'Project [text#1032, 'Emotion, words#1073, filtered_words#1118, stemmed_words#1162, raw_features_array#1426, tfidf_features_array#1471]\n",
      "+- Project [text#1032, id#808, author#809, subreddit#810, link_id#811, parent_id#812, created_utc#813, rater_id#814, example_very_unclear#815, admiration#816, amusement#817, anger#818, annoyance#819, approval#820, caring#821, confusion#822, curiosity#823, desire#824, disappointment#825, disapproval#826, disgust#827, embarrassment#828, excitement#829, fear#830, ... 20 more fields]\n",
      "   +- Project [text#1032, id#808, author#809, subreddit#810, link_id#811, parent_id#812, created_utc#813, rater_id#814, example_very_unclear#815, admiration#816, amusement#817, anger#818, annoyance#819, approval#820, caring#821, confusion#822, curiosity#823, desire#824, disappointment#825, disapproval#826, disgust#827, embarrassment#828, excitement#829, fear#830, ... 19 more fields]\n",
      "      +- Project [text#1032, id#808, author#809, subreddit#810, link_id#811, parent_id#812, created_utc#813, rater_id#814, example_very_unclear#815, admiration#816, amusement#817, anger#818, annoyance#819, approval#820, caring#821, confusion#822, curiosity#823, desire#824, disappointment#825, disapproval#826, disgust#827, embarrassment#828, excitement#829, fear#830, ... 18 more fields]\n",
      "         +- Project [text#1032, id#808, author#809, subreddit#810, link_id#811, parent_id#812, created_utc#813, rater_id#814, example_very_unclear#815, admiration#816, amusement#817, anger#818, annoyance#819, approval#820, caring#821, confusion#822, curiosity#823, desire#824, disappointment#825, disapproval#826, disgust#827, embarrassment#828, excitement#829, fear#830, ... 17 more fields]\n",
      "            +- Project [text#1032, id#808, author#809, subreddit#810, link_id#811, parent_id#812, created_utc#813, rater_id#814, example_very_unclear#815, admiration#816, amusement#817, anger#818, annoyance#819, approval#820, caring#821, confusion#822, curiosity#823, desire#824, disappointment#825, disapproval#826, disgust#827, embarrassment#828, excitement#829, fear#830, ... 16 more fields]\n",
      "               +- Project [text#1032, id#808, author#809, subreddit#810, link_id#811, parent_id#812, created_utc#813, rater_id#814, example_very_unclear#815, admiration#816, amusement#817, anger#818, annoyance#819, approval#820, caring#821, confusion#822, curiosity#823, desire#824, disappointment#825, disapproval#826, disgust#827, embarrassment#828, excitement#829, fear#830, ... 15 more fields]\n",
      "                  +- Project [text#1032, id#808, author#809, subreddit#810, link_id#811, parent_id#812, created_utc#813, rater_id#814, example_very_unclear#815, admiration#816, amusement#817, anger#818, annoyance#819, approval#820, caring#821, confusion#822, curiosity#823, desire#824, disappointment#825, disapproval#826, disgust#827, embarrassment#828, excitement#829, fear#830, ... 14 more fields]\n",
      "                     +- GlobalLimit 1000\n",
      "                        +- LocalLimit 1000\n",
      "                           +- Project [regexp_replace(text#994, [^a-zA-Z0-9\\s'], , 1) AS text#1032, id#808, author#809, subreddit#810, link_id#811, parent_id#812, created_utc#813, rater_id#814, example_very_unclear#815, admiration#816, amusement#817, anger#818, annoyance#819, approval#820, caring#821, confusion#822, curiosity#823, desire#824, disappointment#825, disapproval#826, disgust#827, embarrassment#828, excitement#829, fear#830, ... 13 more fields]\n",
      "                              +- Project [regexp_replace(text#956, \\bi m\\b, i'm, 1) AS text#994, id#808, author#809, subreddit#810, link_id#811, parent_id#812, created_utc#813, rater_id#814, example_very_unclear#815, admiration#816, amusement#817, anger#818, annoyance#819, approval#820, caring#821, confusion#822, curiosity#823, desire#824, disappointment#825, disapproval#826, disgust#827, embarrassment#828, excitement#829, fear#830, ... 13 more fields]\n",
      "                                 +- Project [lower(text#918) AS text#956, id#808, author#809, subreddit#810, link_id#811, parent_id#812, created_utc#813, rater_id#814, example_very_unclear#815, admiration#816, amusement#817, anger#818, annoyance#819, approval#820, caring#821, confusion#822, curiosity#823, desire#824, disappointment#825, disapproval#826, disgust#827, embarrassment#828, excitement#829, fear#830, ... 13 more fields]\n",
      "                                    +- Project [trim(text#807, None) AS text#918, id#808, author#809, subreddit#810, link_id#811, parent_id#812, created_utc#813, rater_id#814, example_very_unclear#815, admiration#816, amusement#817, anger#818, annoyance#819, approval#820, caring#821, confusion#822, curiosity#823, desire#824, disappointment#825, disapproval#826, disgust#827, embarrassment#828, excitement#829, fear#830, ... 13 more fields]\n",
      "                                       +- Filter AtLeastNNulls(n, text#807,id#808,author#809,subreddit#810,link_id#811,parent_id#812,created_utc#813,rater_id#814,example_very_unclear#815,admiration#816,amusement#817,anger#818,annoyance#819,approval#820,caring#821,confusion#822,curiosity#823,desire#824,disappointment#825,disapproval#826,disgust#827,embarrassment#828,excitement#829,fear#830,gratitude#831,grief#832,joy#833,love#834,nervousness#835,optimism#836,pride#837,realization#838,relief#839,remorse#840,sadness#841,surprise#842,neutral#843)\n",
      "                                          +- GlobalLimit 10\n",
      "                                             +- LocalLimit 10\n",
      "                                                +- Relation[text#807,id#808,author#809,subreddit#810,link_id#811,parent_id#812,created_utc#813,rater_id#814,example_very_unclear#815,admiration#816,amusement#817,anger#818,annoyance#819,approval#820,caring#821,confusion#822,curiosity#823,desire#824,disappointment#825,disapproval#826,disgust#827,embarrassment#828,excitement#829,fear#830,... 13 more fields] parquet\n",
      "\n",
      "Processing folder: ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_DailyDialog_csv/\n",
      "./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_DailyDialog_csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'sentiment']\n",
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- emotion: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- stemmed_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- tfidf_features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/27 21:26:17 ERROR Executor: Exception in task 0.0 in stage 36.0 (TID 419)]\n",
      "com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:182)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:155)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:105)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:287)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:211)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:476)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:456)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/06/27 21:26:17 WARN TaskSetManager: Lost task 0.0 in stage 36.0 (TID 419) (192.168.1.55 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:182)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:155)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:105)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:287)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:211)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:476)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:456)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/06/27 21:26:17 ERROR TaskSetManager: Task 0 in stage 36.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing folder ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_DailyDialog_csv: An error occurred while calling o496.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 419) (192.168.1.55 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:182)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:155)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:105)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:287)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:211)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:476)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:456)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2303)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2252)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2251)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2251)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2490)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2432)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2421)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n",
      "\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)\n",
      "\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)\n",
      "\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:182)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:155)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:105)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:287)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:211)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:476)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:456)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n",
      "Processing folder: ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_GoEmotions_goemotions_2_csv/\n",
      "./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_GoEmotions_goemotions_2_csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id', 'created_utc', 'rater_id', 'example_very_unclear', 'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- rater_id: string (nullable = true)\n",
      " |-- example_very_unclear: string (nullable = true)\n",
      " |-- admiration: string (nullable = true)\n",
      " |-- amusement: string (nullable = true)\n",
      " |-- anger: string (nullable = true)\n",
      " |-- annoyance: string (nullable = true)\n",
      " |-- approval: string (nullable = true)\n",
      " |-- caring: string (nullable = true)\n",
      " |-- confusion: string (nullable = true)\n",
      " |-- curiosity: string (nullable = true)\n",
      " |-- desire: string (nullable = true)\n",
      " |-- disappointment: string (nullable = true)\n",
      " |-- disapproval: string (nullable = true)\n",
      " |-- disgust: string (nullable = true)\n",
      " |-- embarrassment: string (nullable = true)\n",
      " |-- excitement: string (nullable = true)\n",
      " |-- fear: string (nullable = true)\n",
      " |-- gratitude: string (nullable = true)\n",
      " |-- grief: string (nullable = true)\n",
      " |-- joy: string (nullable = true)\n",
      " |-- love: string (nullable = true)\n",
      " |-- nervousness: string (nullable = true)\n",
      " |-- optimism: string (nullable = true)\n",
      " |-- pride: string (nullable = true)\n",
      " |-- realization: string (nullable = true)\n",
      " |-- relief: string (nullable = true)\n",
      " |-- remorse: string (nullable = true)\n",
      " |-- sadness: string (nullable = true)\n",
      " |-- surprise: string (nullable = true)\n",
      " |-- neutral: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- stemmed_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- tfidf_features: vector (nullable = true)\n",
      "\n",
      "Error processing folder ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_GoEmotions_goemotions_2_csv: cannot resolve '`Emotion`' given input columns: [admiration, amusement, anger, annoyance, approval, author, caring, confusion, created_utc, curiosity, desire, disappointment, disapproval, disgust, embarrassment, example_very_unclear, excitement, fear, filtered_words, gratitude, grief, id, joy, link_id, love, nervousness, neutral, optimism, parent_id, pride, rater_id, raw_features, raw_features_array, realization, relief, remorse, sadness, stemmed_words, subreddit, surprise, text, tfidf_features, tfidf_features_array, words];\n",
      "'Project [text#2824, 'Emotion, words#2865, filtered_words#2910, stemmed_words#2954, raw_features_array#3211, tfidf_features_array#3256]\n",
      "+- Project [text#2824, id#2600, author#2601, subreddit#2602, link_id#2603, parent_id#2604, created_utc#2605, rater_id#2606, example_very_unclear#2607, admiration#2608, amusement#2609, anger#2610, annoyance#2611, approval#2612, caring#2613, confusion#2614, curiosity#2615, desire#2616, disappointment#2617, disapproval#2618, disgust#2619, embarrassment#2620, excitement#2621, fear#2622, ... 20 more fields]\n",
      "   +- Project [text#2824, id#2600, author#2601, subreddit#2602, link_id#2603, parent_id#2604, created_utc#2605, rater_id#2606, example_very_unclear#2607, admiration#2608, amusement#2609, anger#2610, annoyance#2611, approval#2612, caring#2613, confusion#2614, curiosity#2615, desire#2616, disappointment#2617, disapproval#2618, disgust#2619, embarrassment#2620, excitement#2621, fear#2622, ... 19 more fields]\n",
      "      +- Project [text#2824, id#2600, author#2601, subreddit#2602, link_id#2603, parent_id#2604, created_utc#2605, rater_id#2606, example_very_unclear#2607, admiration#2608, amusement#2609, anger#2610, annoyance#2611, approval#2612, caring#2613, confusion#2614, curiosity#2615, desire#2616, disappointment#2617, disapproval#2618, disgust#2619, embarrassment#2620, excitement#2621, fear#2622, ... 18 more fields]\n",
      "         +- Project [text#2824, id#2600, author#2601, subreddit#2602, link_id#2603, parent_id#2604, created_utc#2605, rater_id#2606, example_very_unclear#2607, admiration#2608, amusement#2609, anger#2610, annoyance#2611, approval#2612, caring#2613, confusion#2614, curiosity#2615, desire#2616, disappointment#2617, disapproval#2618, disgust#2619, embarrassment#2620, excitement#2621, fear#2622, ... 17 more fields]\n",
      "            +- Project [text#2824, id#2600, author#2601, subreddit#2602, link_id#2603, parent_id#2604, created_utc#2605, rater_id#2606, example_very_unclear#2607, admiration#2608, amusement#2609, anger#2610, annoyance#2611, approval#2612, caring#2613, confusion#2614, curiosity#2615, desire#2616, disappointment#2617, disapproval#2618, disgust#2619, embarrassment#2620, excitement#2621, fear#2622, ... 16 more fields]\n",
      "               +- Project [text#2824, id#2600, author#2601, subreddit#2602, link_id#2603, parent_id#2604, created_utc#2605, rater_id#2606, example_very_unclear#2607, admiration#2608, amusement#2609, anger#2610, annoyance#2611, approval#2612, caring#2613, confusion#2614, curiosity#2615, desire#2616, disappointment#2617, disapproval#2618, disgust#2619, embarrassment#2620, excitement#2621, fear#2622, ... 15 more fields]\n",
      "                  +- Project [text#2824, id#2600, author#2601, subreddit#2602, link_id#2603, parent_id#2604, created_utc#2605, rater_id#2606, example_very_unclear#2607, admiration#2608, amusement#2609, anger#2610, annoyance#2611, approval#2612, caring#2613, confusion#2614, curiosity#2615, desire#2616, disappointment#2617, disapproval#2618, disgust#2619, embarrassment#2620, excitement#2621, fear#2622, ... 14 more fields]\n",
      "                     +- GlobalLimit 1000\n",
      "                        +- LocalLimit 1000\n",
      "                           +- Project [regexp_replace(text#2786, [^a-zA-Z0-9\\s'], , 1) AS text#2824, id#2600, author#2601, subreddit#2602, link_id#2603, parent_id#2604, created_utc#2605, rater_id#2606, example_very_unclear#2607, admiration#2608, amusement#2609, anger#2610, annoyance#2611, approval#2612, caring#2613, confusion#2614, curiosity#2615, desire#2616, disappointment#2617, disapproval#2618, disgust#2619, embarrassment#2620, excitement#2621, fear#2622, ... 13 more fields]\n",
      "                              +- Project [regexp_replace(text#2748, \\bi m\\b, i'm, 1) AS text#2786, id#2600, author#2601, subreddit#2602, link_id#2603, parent_id#2604, created_utc#2605, rater_id#2606, example_very_unclear#2607, admiration#2608, amusement#2609, anger#2610, annoyance#2611, approval#2612, caring#2613, confusion#2614, curiosity#2615, desire#2616, disappointment#2617, disapproval#2618, disgust#2619, embarrassment#2620, excitement#2621, fear#2622, ... 13 more fields]\n",
      "                                 +- Project [lower(text#2710) AS text#2748, id#2600, author#2601, subreddit#2602, link_id#2603, parent_id#2604, created_utc#2605, rater_id#2606, example_very_unclear#2607, admiration#2608, amusement#2609, anger#2610, annoyance#2611, approval#2612, caring#2613, confusion#2614, curiosity#2615, desire#2616, disappointment#2617, disapproval#2618, disgust#2619, embarrassment#2620, excitement#2621, fear#2622, ... 13 more fields]\n",
      "                                    +- Project [trim(text#2599, None) AS text#2710, id#2600, author#2601, subreddit#2602, link_id#2603, parent_id#2604, created_utc#2605, rater_id#2606, example_very_unclear#2607, admiration#2608, amusement#2609, anger#2610, annoyance#2611, approval#2612, caring#2613, confusion#2614, curiosity#2615, desire#2616, disappointment#2617, disapproval#2618, disgust#2619, embarrassment#2620, excitement#2621, fear#2622, ... 13 more fields]\n",
      "                                       +- Filter AtLeastNNulls(n, text#2599,id#2600,author#2601,subreddit#2602,link_id#2603,parent_id#2604,created_utc#2605,rater_id#2606,example_very_unclear#2607,admiration#2608,amusement#2609,anger#2610,annoyance#2611,approval#2612,caring#2613,confusion#2614,curiosity#2615,desire#2616,disappointment#2617,disapproval#2618,disgust#2619,embarrassment#2620,excitement#2621,fear#2622,gratitude#2623,grief#2624,joy#2625,love#2626,nervousness#2627,optimism#2628,pride#2629,realization#2630,relief#2631,remorse#2632,sadness#2633,surprise#2634,neutral#2635)\n",
      "                                          +- GlobalLimit 10\n",
      "                                             +- LocalLimit 10\n",
      "                                                +- Relation[text#2599,id#2600,author#2601,subreddit#2602,link_id#2603,parent_id#2604,created_utc#2605,rater_id#2606,example_very_unclear#2607,admiration#2608,amusement#2609,anger#2610,annoyance#2611,approval#2612,caring#2613,confusion#2614,curiosity#2615,desire#2616,disappointment#2617,disapproval#2618,disgust#2619,embarrassment#2620,excitement#2621,fear#2622,... 13 more fields] parquet\n",
      "\n",
      "Processing folder: ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_Emotion_classify_Data_csv/\n",
      "./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_Emotion_classify_Data_csv\n",
      "['Comment', 'Emotion']\n",
      "Error processing folder ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_Emotion_classify_Data_csv: text does not exist. Available: Comment, emotion\n",
      "Processing folder: ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_GoEmotions_goemotions_1_csv/\n",
      "./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_GoEmotions_goemotions_1_csv\n",
      "['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id', 'created_utc', 'rater_id', 'example_very_unclear', 'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- rater_id: string (nullable = true)\n",
      " |-- example_very_unclear: string (nullable = true)\n",
      " |-- admiration: string (nullable = true)\n",
      " |-- amusement: string (nullable = true)\n",
      " |-- anger: string (nullable = true)\n",
      " |-- annoyance: string (nullable = true)\n",
      " |-- approval: string (nullable = true)\n",
      " |-- caring: string (nullable = true)\n",
      " |-- confusion: string (nullable = true)\n",
      " |-- curiosity: string (nullable = true)\n",
      " |-- desire: string (nullable = true)\n",
      " |-- disappointment: string (nullable = true)\n",
      " |-- disapproval: string (nullable = true)\n",
      " |-- disgust: string (nullable = true)\n",
      " |-- embarrassment: string (nullable = true)\n",
      " |-- excitement: string (nullable = true)\n",
      " |-- fear: string (nullable = true)\n",
      " |-- gratitude: string (nullable = true)\n",
      " |-- grief: string (nullable = true)\n",
      " |-- joy: string (nullable = true)\n",
      " |-- love: string (nullable = true)\n",
      " |-- nervousness: string (nullable = true)\n",
      " |-- optimism: string (nullable = true)\n",
      " |-- pride: string (nullable = true)\n",
      " |-- realization: string (nullable = true)\n",
      " |-- relief: string (nullable = true)\n",
      " |-- remorse: string (nullable = true)\n",
      " |-- sadness: string (nullable = true)\n",
      " |-- surprise: string (nullable = true)\n",
      " |-- neutral: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- stemmed_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- tfidf_features: vector (nullable = true)\n",
      "\n",
      "Error processing folder ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_GoEmotions_goemotions_1_csv: cannot resolve '`Emotion`' given input columns: [admiration, amusement, anger, annoyance, approval, author, caring, confusion, created_utc, curiosity, desire, disappointment, disapproval, disgust, embarrassment, example_very_unclear, excitement, fear, filtered_words, gratitude, grief, id, joy, link_id, love, nervousness, neutral, optimism, parent_id, pride, rater_id, raw_features, raw_features_array, realization, relief, remorse, sadness, stemmed_words, subreddit, surprise, text, tfidf_features, tfidf_features_array, words];\n",
      "'Project [text#3925, 'Emotion, words#3966, filtered_words#4011, stemmed_words#4055, raw_features_array#4312, tfidf_features_array#4357]\n",
      "+- Project [text#3925, id#3701, author#3702, subreddit#3703, link_id#3704, parent_id#3705, created_utc#3706, rater_id#3707, example_very_unclear#3708, admiration#3709, amusement#3710, anger#3711, annoyance#3712, approval#3713, caring#3714, confusion#3715, curiosity#3716, desire#3717, disappointment#3718, disapproval#3719, disgust#3720, embarrassment#3721, excitement#3722, fear#3723, ... 20 more fields]\n",
      "   +- Project [text#3925, id#3701, author#3702, subreddit#3703, link_id#3704, parent_id#3705, created_utc#3706, rater_id#3707, example_very_unclear#3708, admiration#3709, amusement#3710, anger#3711, annoyance#3712, approval#3713, caring#3714, confusion#3715, curiosity#3716, desire#3717, disappointment#3718, disapproval#3719, disgust#3720, embarrassment#3721, excitement#3722, fear#3723, ... 19 more fields]\n",
      "      +- Project [text#3925, id#3701, author#3702, subreddit#3703, link_id#3704, parent_id#3705, created_utc#3706, rater_id#3707, example_very_unclear#3708, admiration#3709, amusement#3710, anger#3711, annoyance#3712, approval#3713, caring#3714, confusion#3715, curiosity#3716, desire#3717, disappointment#3718, disapproval#3719, disgust#3720, embarrassment#3721, excitement#3722, fear#3723, ... 18 more fields]\n",
      "         +- Project [text#3925, id#3701, author#3702, subreddit#3703, link_id#3704, parent_id#3705, created_utc#3706, rater_id#3707, example_very_unclear#3708, admiration#3709, amusement#3710, anger#3711, annoyance#3712, approval#3713, caring#3714, confusion#3715, curiosity#3716, desire#3717, disappointment#3718, disapproval#3719, disgust#3720, embarrassment#3721, excitement#3722, fear#3723, ... 17 more fields]\n",
      "            +- Project [text#3925, id#3701, author#3702, subreddit#3703, link_id#3704, parent_id#3705, created_utc#3706, rater_id#3707, example_very_unclear#3708, admiration#3709, amusement#3710, anger#3711, annoyance#3712, approval#3713, caring#3714, confusion#3715, curiosity#3716, desire#3717, disappointment#3718, disapproval#3719, disgust#3720, embarrassment#3721, excitement#3722, fear#3723, ... 16 more fields]\n",
      "               +- Project [text#3925, id#3701, author#3702, subreddit#3703, link_id#3704, parent_id#3705, created_utc#3706, rater_id#3707, example_very_unclear#3708, admiration#3709, amusement#3710, anger#3711, annoyance#3712, approval#3713, caring#3714, confusion#3715, curiosity#3716, desire#3717, disappointment#3718, disapproval#3719, disgust#3720, embarrassment#3721, excitement#3722, fear#3723, ... 15 more fields]\n",
      "                  +- Project [text#3925, id#3701, author#3702, subreddit#3703, link_id#3704, parent_id#3705, created_utc#3706, rater_id#3707, example_very_unclear#3708, admiration#3709, amusement#3710, anger#3711, annoyance#3712, approval#3713, caring#3714, confusion#3715, curiosity#3716, desire#3717, disappointment#3718, disapproval#3719, disgust#3720, embarrassment#3721, excitement#3722, fear#3723, ... 14 more fields]\n",
      "                     +- GlobalLimit 1000\n",
      "                        +- LocalLimit 1000\n",
      "                           +- Project [regexp_replace(text#3887, [^a-zA-Z0-9\\s'], , 1) AS text#3925, id#3701, author#3702, subreddit#3703, link_id#3704, parent_id#3705, created_utc#3706, rater_id#3707, example_very_unclear#3708, admiration#3709, amusement#3710, anger#3711, annoyance#3712, approval#3713, caring#3714, confusion#3715, curiosity#3716, desire#3717, disappointment#3718, disapproval#3719, disgust#3720, embarrassment#3721, excitement#3722, fear#3723, ... 13 more fields]\n",
      "                              +- Project [regexp_replace(text#3849, \\bi m\\b, i'm, 1) AS text#3887, id#3701, author#3702, subreddit#3703, link_id#3704, parent_id#3705, created_utc#3706, rater_id#3707, example_very_unclear#3708, admiration#3709, amusement#3710, anger#3711, annoyance#3712, approval#3713, caring#3714, confusion#3715, curiosity#3716, desire#3717, disappointment#3718, disapproval#3719, disgust#3720, embarrassment#3721, excitement#3722, fear#3723, ... 13 more fields]\n",
      "                                 +- Project [lower(text#3811) AS text#3849, id#3701, author#3702, subreddit#3703, link_id#3704, parent_id#3705, created_utc#3706, rater_id#3707, example_very_unclear#3708, admiration#3709, amusement#3710, anger#3711, annoyance#3712, approval#3713, caring#3714, confusion#3715, curiosity#3716, desire#3717, disappointment#3718, disapproval#3719, disgust#3720, embarrassment#3721, excitement#3722, fear#3723, ... 13 more fields]\n",
      "                                    +- Project [trim(text#3700, None) AS text#3811, id#3701, author#3702, subreddit#3703, link_id#3704, parent_id#3705, created_utc#3706, rater_id#3707, example_very_unclear#3708, admiration#3709, amusement#3710, anger#3711, annoyance#3712, approval#3713, caring#3714, confusion#3715, curiosity#3716, desire#3717, disappointment#3718, disapproval#3719, disgust#3720, embarrassment#3721, excitement#3722, fear#3723, ... 13 more fields]\n",
      "                                       +- Filter AtLeastNNulls(n, text#3700,id#3701,author#3702,subreddit#3703,link_id#3704,parent_id#3705,created_utc#3706,rater_id#3707,example_very_unclear#3708,admiration#3709,amusement#3710,anger#3711,annoyance#3712,approval#3713,caring#3714,confusion#3715,curiosity#3716,desire#3717,disappointment#3718,disapproval#3719,disgust#3720,embarrassment#3721,excitement#3722,fear#3723,gratitude#3724,grief#3725,joy#3726,love#3727,nervousness#3728,optimism#3729,pride#3730,realization#3731,relief#3732,remorse#3733,sadness#3734,surprise#3735,neutral#3736)\n",
      "                                          +- GlobalLimit 10\n",
      "                                             +- LocalLimit 10\n",
      "                                                +- Relation[text#3700,id#3701,author#3702,subreddit#3703,link_id#3704,parent_id#3705,created_utc#3706,rater_id#3707,example_very_unclear#3708,admiration#3709,amusement#3710,anger#3711,annoyance#3712,approval#3713,caring#3714,confusion#3715,curiosity#3716,desire#3717,disappointment#3718,disapproval#3719,disgust#3720,embarrassment#3721,excitement#3722,fear#3723,... 13 more fields] parquet\n",
      "\n",
      "Processing folder: ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_tweet_sentiment_csv/\n",
      "./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_tweet_sentiment_csv\n",
      "['tweet', 'sentiment']\n",
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- emotion: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- stemmed_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- tfidf_features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/27 21:26:57 ERROR Executor: Exception in task 0.0 in stage 85.0 (TID 945)]\n",
      "com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:182)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:155)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:105)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:287)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:211)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:476)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:456)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/06/27 21:26:57 WARN TaskSetManager: Lost task 0.0 in stage 85.0 (TID 945) (192.168.1.55 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:182)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:155)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:105)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:287)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:211)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:476)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:456)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/06/27 21:26:57 ERROR TaskSetManager: Task 0 in stage 85.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing folder ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_tweet_sentiment_csv: An error occurred while calling o1177.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 85.0 failed 1 times, most recent failure: Lost task 0.0 in stage 85.0 (TID 945) (192.168.1.55 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:182)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:155)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:105)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:287)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:211)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:476)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:456)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2303)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2252)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2251)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2251)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2490)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2432)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2421)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n",
      "\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)\n",
      "\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)\n",
      "\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:182)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:155)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:105)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:287)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:211)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:476)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:456)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n",
      "Processing folder: ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_emotion_sentimen_dataset_csv/\n",
      "./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_emotion_sentimen_dataset_csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', 'text', 'Emotion']\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- emotion: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- stemmed_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- tfidf_features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/27 21:27:30 ERROR Executor: Exception in task 0.0 in stage 104.0 (TID 1155)\n",
      "com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:182)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:155)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:105)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:287)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:211)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:476)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:456)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/06/27 21:27:31 WARN TaskSetManager: Lost task 0.0 in stage 104.0 (TID 1155) (192.168.1.55 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:182)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:155)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:105)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:287)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:211)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:476)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:456)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/06/27 21:27:31 ERROR TaskSetManager: Task 0 in stage 104.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing folder ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_emotion_sentimen_dataset_csv: An error occurred while calling o1403.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 104.0 failed 1 times, most recent failure: Lost task 0.0 in stage 104.0 (TID 1155) (192.168.1.55 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:182)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:155)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:105)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:287)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:211)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:476)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:456)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2303)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2252)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2251)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2251)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2490)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2432)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2421)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n",
      "\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)\n",
      "\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)\n",
      "\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:182)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:155)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:105)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:287)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:211)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:476)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:456)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n",
      "Processing folder: ./../../../delta_lake/Kaggle/___________datasets_Kaggle_parquet_train-00000-of-00001_parquet/\n",
      "./../../../delta_lake/Kaggle/___________datasets_Kaggle_parquet_train-00000-of-00001_parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label']\n",
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- stemmed_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- tfidf_features: vector (nullable = true)\n",
      "\n",
      "Error processing folder ./../../../delta_lake/Kaggle/___________datasets_Kaggle_parquet_train-00000-of-00001_parquet: cannot resolve '`Emotion`' given input columns: [filtered_words, label, raw_features, raw_features_array, stemmed_words, text, tfidf_features, tfidf_features_array, words];\n",
      "'Project [text#6215, 'Emotion, words#6221, filtered_words#6231, stemmed_words#6240, raw_features_array#6392, tfidf_features_array#6402]\n",
      "+- Project [text#6215, label#6201L, words#6221, filtered_words#6231, stemmed_words#6240, raw_features#6308, tfidf_features#6380, raw_features_array#6392, vector_to_array(tfidf_features#6380) AS tfidf_features_array#6402]\n",
      "   +- Project [text#6215, label#6201L, words#6221, filtered_words#6231, stemmed_words#6240, raw_features#6308, tfidf_features#6380, vector_to_array(raw_features#6308) AS raw_features_array#6392]\n",
      "      +- Project [text#6215, label#6201L, words#6221, filtered_words#6231, stemmed_words#6240, raw_features#6308, UDF(raw_features#6308) AS tfidf_features#6380]\n",
      "         +- Project [text#6215, label#6201L, words#6221, filtered_words#6231, stemmed_words#6240, UDF(stemmed_words#6240) AS raw_features#6308]\n",
      "            +- Project [text#6215, label#6201L, words#6221, filtered_words#6231, stem_tokens(filtered_words#6231) AS stemmed_words#6240]\n",
      "               +- Project [text#6215, label#6201L, words#6221, UDF(words#6221) AS filtered_words#6231]\n",
      "                  +- Project [text#6215, label#6201L, UDF(text#6215) AS words#6221]\n",
      "                     +- GlobalLimit 1000\n",
      "                        +- LocalLimit 1000\n",
      "                           +- Project [regexp_replace(text#6212, [^a-zA-Z0-9\\s'], , 1) AS text#6215, label#6201L]\n",
      "                              +- Project [regexp_replace(text#6209, \\bi m\\b, i'm, 1) AS text#6212, label#6201L]\n",
      "                                 +- Project [lower(text#6206) AS text#6209, label#6201L]\n",
      "                                    +- Project [trim(text#6200, None) AS text#6206, label#6201L]\n",
      "                                       +- Filter AtLeastNNulls(n, text#6200,label#6201L)\n",
      "                                          +- GlobalLimit 10\n",
      "                                             +- LocalLimit 10\n",
      "                                                +- Relation[text#6200,label#6201L] parquet\n",
      "\n",
      "Processing folder: ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_twitter_csv/\n",
      "./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_twitter_csv\n",
      "['id', 'label', 'tweet']\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- stemmed_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- tfidf_features: vector (nullable = true)\n",
      "\n",
      "Error processing folder ./../../../delta_lake/Kaggle/___________datasets_Kaggle_csv_twitter_csv: cannot resolve '`Emotion`' given input columns: [filtered_words, id, label, raw_features, raw_features_array, stemmed_words, text, tfidf_features, tfidf_features_array, words];\n",
      "'Project [text#6827, 'Emotion, words#6834, filtered_words#6845, stemmed_words#6855, raw_features_array#7010, tfidf_features_array#7021]\n",
      "+- Project [id#6802, label#6803, text#6827, words#6834, filtered_words#6845, stemmed_words#6855, raw_features#6924, tfidf_features#6997, raw_features_array#7010, vector_to_array(tfidf_features#6997) AS tfidf_features_array#7021]\n",
      "   +- Project [id#6802, label#6803, text#6827, words#6834, filtered_words#6845, stemmed_words#6855, raw_features#6924, tfidf_features#6997, vector_to_array(raw_features#6924) AS raw_features_array#7010]\n",
      "      +- Project [id#6802, label#6803, text#6827, words#6834, filtered_words#6845, stemmed_words#6855, raw_features#6924, UDF(raw_features#6924) AS tfidf_features#6997]\n",
      "         +- Project [id#6802, label#6803, text#6827, words#6834, filtered_words#6845, stemmed_words#6855, UDF(stemmed_words#6855) AS raw_features#6924]\n",
      "            +- Project [id#6802, label#6803, text#6827, words#6834, filtered_words#6845, stem_tokens(filtered_words#6845) AS stemmed_words#6855]\n",
      "               +- Project [id#6802, label#6803, text#6827, words#6834, UDF(words#6834) AS filtered_words#6845]\n",
      "                  +- Project [id#6802, label#6803, text#6827, UDF(text#6827) AS words#6834]\n",
      "                     +- GlobalLimit 1000\n",
      "                        +- LocalLimit 1000\n",
      "                           +- Project [id#6802, label#6803, regexp_replace(text#6823, [^a-zA-Z0-9\\s'], , 1) AS text#6827]\n",
      "                              +- Project [id#6802, label#6803, regexp_replace(text#6819, \\bi m\\b, i'm, 1) AS text#6823]\n",
      "                                 +- Project [id#6802, label#6803, lower(text#6815) AS text#6819]\n",
      "                                    +- Project [id#6802, label#6803, trim(text#6811, None) AS text#6815]\n",
      "                                       +- Project [id#6802, label#6803, tweet#6804 AS text#6811]\n",
      "                                          +- Filter AtLeastNNulls(n, id#6802,label#6803,tweet#6804)\n",
      "                                             +- GlobalLimit 10\n",
      "                                                +- LocalLimit 10\n",
      "                                                   +- Relation[id#6802,label#6803,tweet#6804] parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_data = \"./../../../delta_lake/\"\n",
    "\n",
    "for source in sources:\n",
    "    path = os.path.join(path_data, source)\n",
    "\n",
    "    if os.path.isdir(path):\n",
    "        for folder in os.listdir(path):\n",
    "            full_path = os.path.join(path, folder)\n",
    "            print(f\"Processing folder: {full_path}/\")\n",
    "\n",
    "            try:\n",
    "                df_csv = readFromDeltaLake(full_path)\n",
    "\n",
    "                df_csv_clean = cleanCSV(df_csv) #  .limit(10) Limit to 10 rows for performance\n",
    "                df_stemmed = tokenizer(df_csv_clean)\n",
    "                df_tfidf = tf_idf(df_stemmed)\n",
    "                df_tfidf_safe = df_clean(df_tfidf)\n",
    "\n",
    "                storeInMongoDB(df_tfidf_safe)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing folder {full_path}: {e}\")\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
